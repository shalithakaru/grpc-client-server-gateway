# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0
# This file is generated by 'make generate-kubernetes-manifests'
---
apiVersion: v1
kind: Namespace
metadata:
  name: poc
---
# Source: opentelemetry-demo/charts/opensearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "demo-opensearch-pdb"
  labels:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.15.0"
    app.kubernetes.io/component: demo-opensearch
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
  name: opentelemetry-demo-grafana
  namespace: poc
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-demo-jaeger
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: all-in-one
automountServiceAccountToken: true
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-demo-otelcol
  namespace: poc
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
---
# Source: opentelemetry-demo/charts/prometheus/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
  namespace: poc
  annotations:
    {}
---
# Source: opentelemetry-demo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-demo
  labels:
    
    opentelemetry.io/name: opentelemetry-demo
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/name: opentelemetry-demo
    app.kubernetes.io/version: "1.11.1"
    app.kubernetes.io/part-of: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "YWRtaW4="
  ldap-toml: ""
---
# Source: opentelemetry-demo/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
data:
  
  plugins: grafana-opensearch-datasource
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [auth]
    disable_login_form = true
    [auth.anonymous]
    enabled = true
    org_name = Main Org.
    org_role = Admin
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
    root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana
    serve_from_sub_path = true
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - editable: true
      isDefault: true
      jsonData:
        exemplarTraceIdDestinations:
        - datasourceUid: webstore-traces
          name: trace_id
        - name: trace_id
          url: http://localhost:8080/jaeger/ui/trace/$${__value.raw}
          urlDisplayLabel: View in Jaeger UI
      name: Prometheus
      type: prometheus
      uid: webstore-metrics
      url: http://opentelemetry-demo-prometheus-server:9090
    - editable: true
      isDefault: false
      name: Jaeger
      type: jaeger
      uid: webstore-traces
      url: http://opentelemetry-demo-jaeger-query:16686/jaeger/ui
    - access: proxy
      editable: true
      isDefault: false
      jsonData:
        database: otel
        flavor: opensearch
        logLevelField: severity
        logMessageField: body
        pplEnabled: true
        timeField: observedTimestamp
        version: 2.13.0
      name: OpenSearch
      type: grafana-opensearch-datasource
      url: http://demo-opensearch:9200/
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      editable: true
      folder: ""
      name: default
      options:
        path: /var/lib/grafana/dashboards/default
      orgId: 1
      type: file
---
# Source: opentelemetry-demo/charts/opensearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-opensearch-config
  labels:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.15.0"
    app.kubernetes.io/component: demo-opensearch
data:
  opensearch.yml: |
    cluster.name: opensearch-cluster
    
    # Bind to all interfaces because we don't know what IP address Docker will assign to us.
    network.host: 0.0.0.0
    
    # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
    # Implicitly done if ".singleNode" is set to "true".
    # discovery.type: single-node
    
    # Start OpenSearch Security Demo Configuration
    # WARNING: revise all the lines below before you go into production
    plugins:
      security:
        ssl:
          transport:
            pemcert_filepath: esnode.pem
            pemkey_filepath: esnode-key.pem
            pemtrustedcas_filepath: root-ca.pem
            enforce_hostname_verification: false
          http:
            enabled: true
            pemcert_filepath: esnode.pem
            pemkey_filepath: esnode-key.pem
            pemtrustedcas_filepath: root-ca.pem
        allow_unsafe_democertificates: true
        allow_default_init_securityindex: true
        authcz:
          admin_dn:
            - CN=kirk,OU=client,O=client,L=test,C=de
        audit.type: internal_opensearch
        enable_snapshot_restore_privilege: true
        check_snapshot_restore_write_privileges: true
        restapi:
          roles_enabled: ["all_access", "security_rest_api_access"]
        system_indices:
          enabled: true
          indices:
            [
              ".opendistro-alerting-config",
              ".opendistro-alerting-alert*",
              ".opendistro-anomaly-results*",
              ".opendistro-anomaly-detector*",
              ".opendistro-anomaly-checkpoints",
              ".opendistro-anomaly-detection-state",
              ".opendistro-reports-*",
              ".opendistro-notifications-*",
              ".opendistro-notebooks",
              ".opendistro-asynchronous-search-response*",
            ]
    ######## End OpenSearch Security Demo Configuration ########
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-demo-otelcol
  namespace: poc
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
    
data:
  relay: |
    connectors:
      spanmetrics: {}
    exporters:
      debug: {}
      opensearch:
        http:
          endpoint: http://demo-opensearch:9200
          tls:
            insecure: true
        logs_index: otel
      otlp:
        endpoint: 'opentelemetry-demo-jaeger-collector:4317'
        tls:
          insecure: true
      otlphttp/prometheus:
        endpoint: http://opentelemetry-demo-prometheus-server:9090/api/v1/otlp
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch: {}
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      resource:
        attributes:
        - action: insert
          from_attribute: k8s.pod.uid
          key: service.instance.id
      transform:
        error_mode: ignore
        trace_statements:
        - context: span
          statements:
          - replace_pattern(name, "\\?.*", "")
          - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")
    receivers:
      httpcheck/frontendproxy:
        targets:
        - endpoint: http://opentelemetry-demo-frontendproxy:8080
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            cors:
              allowed_origins:
              - http://*
              - https://*
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      redis:
        collection_interval: 10s
        endpoint: valkey-cart:6379
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      pipelines:
        logs:
          exporters:
          - opensearch
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - resource
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - otlphttp/prometheus
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - resource
          - batch
          receivers:
          - httpcheck/frontendproxy
          - redis
          - otlp
          - spanmetrics
        traces:
          exporters:
          - otlp
          - debug
          - spanmetrics
          processors:
          - k8sattributes
          - memory_limiter
          - resource
          - transform
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
---
# Source: opentelemetry-demo/charts/prometheus/templates/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
  namespace: poc
data:
  allow-snippet-annotations: "false"
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 30s
      scrape_interval: 5s
      scrape_timeout: 3s
    storage:
      tsdb:
        out_of_order_time_window: 30m
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - honor_labels: true
      job_name: otel-collector
      kubernetes_sd_configs:
      - namespaces:
          own_namespace: true
        role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_opentelemetry_community_demo
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: opentelemetry-demo/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
  name: opentelemetry-demo-grafana-clusterrole
rules: []
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opentelemetry-demo-otelcol
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
    
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "discovery.k8s.io"
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: opentelemetry-demo/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: opentelemetry-demo-grafana-clusterrolebinding
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
subjects:
  - kind: ServiceAccount
    name: opentelemetry-demo-grafana
    namespace: poc
roleRef:
  kind: ClusterRole
  name: opentelemetry-demo-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opentelemetry-demo-otelcol
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
    
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-demo-otelcol
subjects:
- kind: ServiceAccount
  name: opentelemetry-demo-otelcol
  namespace: poc
---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
subjects:
  - kind: ServiceAccount
    name: opentelemetry-demo-prometheus-server
    namespace: poc
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-demo-prometheus-server
---
# Source: opentelemetry-demo/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
rules: []
---
# Source: opentelemetry-demo/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: opentelemetry-demo-grafana
subjects:
- kind: ServiceAccount
  name: opentelemetry-demo-grafana
  namespace: poc
---
# Source: opentelemetry-demo/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
spec:
  type: NodePort
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
      nodePort: 30000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-agent-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-jaeger-agent
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-agent
spec:
  clusterIP: None
  ports:
    - name: zk-compact-trft
      port: 5775
      protocol: UDP
      targetPort: 0
    - name: config-rest
      port: 5778
      targetPort: 0
    - name: jg-compact-trft
      port: 6831
      protocol: UDP
      targetPort: 0
    - name: jg-binary-trft
      port: 6832
      protocol: UDP
      targetPort: 0
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-collector-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-jaeger-collector
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-collector
spec:
  clusterIP: None
  ports:
    - name: http-zipkin
      port: 9411
      targetPort: 0
      appProtocol: http
    - name: grpc-http
      port: 14250
      targetPort: 0
      appProtocol: grpc
    - name: c-tchan-trft
      port: 14267
      targetPort: 0
    - name: http-c-binary-trft
      port: 14268
      targetPort: 0
      appProtocol: http
    - name: otlp-grpc
      port: 4317
      targetPort: 0
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 0
      appProtocol: http
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-query-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-jaeger-query
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-query
spec:
  clusterIP: None
  ports:
    - name: http-query
      port: 16686
      targetPort: 16686
    - name: grpc-query
      port: 16685
      targetPort: 16685
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: demo-opensearch
  labels:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.15.0"
    app.kubernetes.io/component: demo-opensearch
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: opentelemetry-demo/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: demo-opensearch-headless
  labels:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.15.0"
    app.kubernetes.io/component: demo-opensearch
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like opensearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
  - name: metrics
    port: 9600
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-otelcol
  namespace: poc
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
    
    component: standalone-collector
spec:
  type: NodePort
  ports:
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: metrics
      port: 8888
      targetPort: 8888
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
      nodePort: 30017
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
      nodePort: 30018
    - name: prometheus
      port: 9464
      targetPort: 9464
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: opentelemetry-demo/charts/prometheus/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
  namespace: poc
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
      nodePort: 30090
  selector:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
  sessionAffinity: None
  type: "NodePort"
---
# Source: opentelemetry-demo/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-grafana
  namespace: poc
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: opentelemetry-demo
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: opentelemetry-demo
      annotations:
        checksum/config: 61c1235cb51410dbf2f50b9e64c763f974431188e72518b56a21adb2c7ff6514
        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
        checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: opentelemetry-demo-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:11.1.0"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: dashboards-default
              mountPath: "/var/lib/grafana/dashboards/default"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
            - name: config
              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
              subPath: "dashboardproviders.yaml"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: opentelemetry-demo-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: opentelemetry-demo-grafana
                  key: admin-password
            - name: GF_INSTALL_PLUGINS
              valueFrom:
                configMapKeyRef:
                  name: opentelemetry-demo-grafana
                  key: plugins
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            limits:
              memory: 150Mi
      volumes:
        - name: config
          configMap:
            name: opentelemetry-demo-grafana
        - name: dashboards-default
          configMap:
            name: opentelemetry-demo-grafana-dashboards
        - name: storage
          emptyDir: {}
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-jaeger
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: all-in-one
    prometheus.io/port: "14269"
    prometheus.io/scrape: "true"
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: opentelemetry-demo
      app.kubernetes.io/component: all-in-one
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: all-in-one
      annotations:
        prometheus.io/port: "14269"
        prometheus.io/scrape: "true"
    spec:
      
      containers:
        - env:
            - name: METRICS_STORAGE_TYPE
              value: prometheus
            - name: SPAN_STORAGE_TYPE
              value: memory
            
            - name: COLLECTOR_ZIPKIN_HOST_PORT
              value: :9411
            - name: JAEGER_DISABLED
              value: "false"
            - name: COLLECTOR_OTLP_ENABLED
              value: "true"
          securityContext:
            {}
          image: jaegertracing/all-in-one:1.53.0
          imagePullPolicy: IfNotPresent
          name: jaeger
          args:
            - "--memory.max-traces=5000"
            - "--query.base-path=/jaeger/ui"
            - "--prometheus.server-url=http://opentelemetry-demo-prometheus-server:9090"
            - "--prometheus.query.normalize-calls=true"
            - "--prometheus.query.normalize-duration=true"
          ports:
            - containerPort: 5775
              protocol: UDP
            - containerPort: 6831
              protocol: UDP
            - containerPort: 6832
              protocol: UDP
            - containerPort: 5778
              protocol: TCP
            - containerPort: 16686
              protocol: TCP
            - containerPort: 16685
              protocol: TCP
            - containerPort: 9411
              protocol: TCP
            - containerPort: 4317
              protocol: TCP
            - containerPort: 4318
              protocol: TCP
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 14269
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 14269
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 400Mi
          volumeMounts:
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsUser: 10001
      serviceAccountName: opentelemetry-demo-jaeger
      volumes:
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-otelcol
  namespace: poc
  labels:
    app.kubernetes.io/name: otelcol
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.105.0"
    
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: otelcol
      app.kubernetes.io/instance: opentelemetry-demo
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 18c43f1d8e381b9537a11dc3fac61916f789141ca14acfbf86f15fcad446868d
        opentelemetry_community_demo: "true"
        prometheus.io/port: "9464"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: otelcol
        app.kubernetes.io/instance: opentelemetry-demo
        component: standalone-collector
        
    spec:
      
      serviceAccountName: opentelemetry-demo-otelcol
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          args:
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.105.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: metrics
              containerPort: 8888
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: prometheus
              containerPort: 9464
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: GOMEMLIMIT
              value: "160MiB"
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              memory: 200Mi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: opentelemetry-demo-otelcol
            items:
              - key: relay
                path: relay.yaml
      hostNetwork: false
---
# Source: opentelemetry-demo/charts/prometheus/templates/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v2.53.1
    app.kubernetes.io/part-of: prometheus
  name: opentelemetry-demo-prometheus-server
  namespace: poc
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: opentelemetry-demo
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: prometheus
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/version: v2.53.1
        app.kubernetes.io/part-of: prometheus
    spec:
      enableServiceLinks: true
      serviceAccountName: opentelemetry-demo-prometheus-server
      containers:

        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:v2.53.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --enable-feature=exemplar-storage
            - --enable-feature=otlp-write-receiver
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: opentelemetry-demo-prometheus-server
        - name: storage-volume
          emptyDir:
            {}
---
# Source: opentelemetry-demo/charts/opensearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: demo-opensearch
  labels:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.15.0"
    app.kubernetes.io/component: demo-opensearch
  annotations:
    majorVersion: "2"
spec:
  serviceName: demo-opensearch-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: opentelemetry-demo
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "demo-opensearch"
      labels:
        app.kubernetes.io/name: opensearch
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/version: "2.15.0"
        app.kubernetes.io/component: demo-opensearch
      annotations:
        configchecksum: 3b3e25ff4d35eda228b1cd06d8a76d454abc7356a87dc11224b077fa79a69e2
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: false
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/instance
                  operator: In
                  values:
                  - opentelemetry-demo
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - opensearch
      terminationGracePeriodSeconds: 120
      volumes:
      - name: config
        configMap:
          name: demo-opensearch-config
      - emptyDir: {}
        name: config-emptydir
      enableServiceLinks: true
      initContainers:
      - name: configfile
        image: "opensearchproject/opensearch:2.15.0"
        imagePullPolicy: "IfNotPresent"
        command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash
          cp -r /tmp/configfolder/*  /tmp/config/
        resources:
          {}
        volumeMounts:
          - mountPath: /tmp/config/
            name: config-emptydir
          - name: config
            mountPath: /tmp/configfolder/opensearch.yml
            subPath: opensearch.yml
      containers:
      - name: "opensearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000

        image: "opensearchproject/opensearch:2.15.0"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          failureThreshold: 3
          periodSeconds: 5
          tcpSocket:
            port: 9200
          timeoutSeconds: 3
        startupProbe:
          failureThreshold: 30
          initialDelaySeconds: 5
          periodSeconds: 10
          tcpSocket:
            port: 9200
          timeoutSeconds: 3
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        - name: metrics
          containerPort: 9600
        resources:
          limits:
            memory: 1Gi
          requests:
            cpu: 1000m
            memory: 100Mi
        env:
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "opensearch-cluster-master-headless"
        - name: cluster.name
          value: "demo-cluster"
        - name: network.host
          value: "0.0.0.0"
        - name: OPENSEARCH_JAVA_OPTS
          value: "-Xms300m -Xmx300m"
        - name: node.roles
          value: "master,ingest,data,remote_cluster_client,"
        - name: discovery.type
          value: "single-node"
        - name: bootstrap.memory_lock
          value: "true"
        - name: DISABLE_INSTALL_DEMO_CONFIG
          value: "true"
        - name: DISABLE_SECURITY_PLUGIN
          value: "true"
        volumeMounts:
        - name: config-emptydir
          mountPath: /usr/share/opensearch/config/opensearch.yml
          subPath: opensearch.yml
---
# Source: opentelemetry-demo/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
  name: opentelemetry-demo-grafana-test
  namespace: poc
  annotations:
---
# Source: opentelemetry-demo/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-demo-grafana-test
  namespace: poc
  annotations:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
data:
  run.sh: |-
    @test "Test Health" {
      url="http://opentelemetry-demo-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: opentelemetry-demo/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: opentelemetry-demo-grafana-test
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.1.0"
  annotations:
  namespace: poc
spec:
  serviceAccountName: opentelemetry-demo-grafana-test
  containers:
    - name: opentelemetry-demo-test
      image: "docker.io/bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
    - name: tests
      configMap:
        name: opentelemetry-demo-grafana-test
  restartPolicy: Never
  